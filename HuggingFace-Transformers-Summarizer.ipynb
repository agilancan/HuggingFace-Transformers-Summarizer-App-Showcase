{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D6s0-isqRJBm"
      },
      "outputs": [],
      "source": [
        "# Installing relevant libraries\n",
        "!pip --quiet install transformers gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libaries\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "9KTEWGkUvdtD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of supported tasks, in the Huggingface pipelines\n",
        "from transformers.pipelines import PIPELINE_REGISTRY\n",
        "\n",
        "print(PIPELINE_REGISTRY.get_supported_tasks())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_L9dhtnItpV",
        "outputId": "328bc4da-07d6-4e58-8ee0-6c4462f782ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some sample text to summarize to see if model works\n",
        "text = \"The Toronto Raptors have picked up the 2026-27 team option on Darko Rajakovic’s contract per Michael Grange. This move signifies a substantial vote of confidence in the second year head coach as they are locking him up for the next several seasons. Toronto is in the midst of a rebuild currently, and although the team’s record is not positive, and hasn’t been for a couple of seasons, there are clear indicators that Rajakovic is a more than capable coach. RJ Barrett playing the best basketball of his career in Toronto is a direct result of Rajakovic’s offensive scheme, which focuses on using motion to create easy and effective looks. Rajakovic’s defense scheme has also worked wonders this season, as Toronto has risen to 14th in defensive rating this year. His emphasis on pressuring the ball, while also having the backend rotations be cohesive have empowered this often injured Raptors team to be a stout defensive unit. This contract option pick up shows that Rajakovic has proved that he can be trusted to be the leader of this rebuild, and that the front office believes he will succeed in making Toronto better. With this season almost coming to a close, it will cap off Rajakovic’s second season as the head coach of the team.\""
      ],
      "metadata": {
        "id": "WnG1n7bR9Hr2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model being used as default\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "print(summarizer.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZyC2RmxIh9j",
        "outputId": "19fea3c2-22c2-473e-fbb9-a94eedd37337"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BartForConditionalGeneration(\n",
            "  (model): BartModel(\n",
            "    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
            "    (encoder): BartEncoder(\n",
            "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x BartEncoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): GELUActivation()\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): BartDecoder(\n",
            "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x BartDecoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (activation_fn): GELUActivation()\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the PEGASUS tokenizer to help with long text.\n",
        "# Unlike BART, PEGASUS is pre-trained for abstractive(rewrites text in a new way) summarization and can handle longer documents more effectively. Can handle longer sequences (~1024-2048 tokens, depending on variant)\n",
        "# Tokenization helps process large text efficiently, ensuring input sequences fit within the model's token limit.\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmFgERyiMsMf",
        "outputId": "0dfd64fd-6069-4668-e008-2f9834b9d574"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the google/pegasus-large model, while sorting for top downloaded summarization models on the Hugging Face website. As it is highly effective for abstractive summarization tasks. The Pegasus model is pre-trained on a large-scale dataset and fine-tuned for a wide variety of summarization tasks, making it suitable for generating high-quality summaries.\n",
        "summarize = pipeline(\"summarization\", model=\"google/pegasus-large\", tokenizer=tokenizer)\n",
        "\n",
        "def summarization(text):\n",
        "  summarization_results = summarize(text, max_length=300, min_length=30, do_sample=False)\n",
        "  return summarization_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jpSzUHowM1y",
        "outputId": "df7e1e22-8efe-41fa-c5a6-b2405afdf7bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarizing the same text\n",
        "print(summarize(text, max_length=250, min_length=30, do_sample=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6BWqq3H-u-E",
        "outputId": "ddfc123f-1f2c-446d-d71c-741c8944b578"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'Toronto is in the midst of a rebuild currently, and although the team’s record is not positive, and hasn’t been for a couple of seasons, there are clear indicators that Rajakovic is a more than capable coach. Rajakovic’s defense scheme has also worked wonders this season, as Toronto has risen to 14th in defensive rating this year. With this season almost coming to a close, it will cap off Rajakovic’s second season as the head coach of the team.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Gradio interface for the demo\n",
        "demo = gr.Interface(\n",
        "    fn=summarization,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter text here...\"),\n",
        "    outputs=\"text\")\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "yHEzUtD9B5PZ",
        "outputId": "9f0b2be4-6d46-493d-94ab-29723b9fb918"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://75c76cbc3b5fca9aff.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://75c76cbc3b5fca9aff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}
